{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f01fa52b",
   "metadata": {},
   "source": [
    "0. 루브릭\n",
    "1) Transformer와 비교해 변경이 필요한 부분을 서술하였다.\n",
    "제출 노트북 파일 앞부분에 텍스트 블럭으로 서술합니다. 변경이 필요한 블럭을 서술합니다.\n",
    "코드블럭에 변경사항을 주석으로 표시합니다.\n",
    "2) 모델의 입력 형태에 맞게 전처리를 수행하였다.\n",
    "Decoder 기반의 생성모델 임을 감안하여 챗봇 데이터를 변형합니다.\n",
    "3) 모델의 입력 블럭을 GPT 논문에 기반하여 수정하였다.\n",
    "모델의 input이 정상적으로 구성되었는지 확인합니다.\n",
    "4) GPT 모델을 정상적으로 구성하였다. (model.summary, model.fit 결과 캡쳐 첨부)\n",
    "노드의 transformer 코드를 수정하여 GPT1 모델을 구성합니다.\n",
    "5) 입력에 따른 출력이 생성되었다.\n",
    "출력 결과물이 수준에 상관없이 모델이 정상적으로 동작하는지 확인합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7d5e7b",
   "metadata": {},
   "source": [
    "# Step 1. 데이터 수집하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f81288",
   "metadata": {},
   "source": [
    "패키지 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a3fde5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf14205",
   "metadata": {},
   "source": [
    "### 데이터 로딩: CSV 파일을 로드하고 필요한 데이터로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873df8f6",
   "metadata": {},
   "source": [
    "데이터 로딩: ChatbotData.csv 파일을 로드하고 파싱하는 코드를 추가해야 합니다.\n",
    "프로젝트의 기존 데이터 로딩 코드는 영화 대화 데이터셋을 로드하는데 사용됩니다. 이를 한국어 챗봇 데이터로 대체해야 합니다.\n",
    "CSV 파일을 읽어들이고 적절한 형식으로 변환하는 작업이 필요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6c1b66",
   "metadata": {},
   "source": [
    "# Step 2. 데이터 전처리하기\n",
    "전처리 함수: 문장에서 필요 없는 문자 제거 및 구두점 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a5d7293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12시 땡!</td>\n",
       "      <td>하루가 또 가네요.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1지망 학교 떨어졌어</td>\n",
       "      <td>위로해 드립니다.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3박4일 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3박4일 정도 놀러가고 싶다</td>\n",
       "      <td>여행은 언제나 좋죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PPL 심하네</td>\n",
       "      <td>눈살이 찌푸려지죠.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Q            A  label\n",
       "0           12시 땡!   하루가 또 가네요.      0\n",
       "1      1지망 학교 떨어졌어    위로해 드립니다.      0\n",
       "2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n",
       "4          PPL 심하네   눈살이 찌푸려지죠.      0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/ChatbotData.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c8271131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12시 땡! $ 하루가 또 가네요.', '1지망 학교 떨어졌어 $ 위로해 드립니다.', '3박4일 놀러가고 싶다 $ 여행은 언제나 좋죠.', '3박4일 정도 놀러가고 싶다 $ 여행은 언제나 좋죠.', 'PPL 심하네 $ 눈살이 찌푸려지죠.']\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    concatenated_text = f\"{row['Q']} $ {row['A']}\"\n",
    "    data.append(concatenated_text)\n",
    "\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2913671a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 전처리 함수\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.strip()\n",
    "    # 구두점과 한글 사이에 공백 추가\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    # 한글과 구두점을 제외한 모든 문자를 공백으로 대체\n",
    "    sentence = re.sub(r\"[^가-힣?0-9?.!,]+\", \" \", sentence) #허용할 부분, 나머지 공백\n",
    "    sentence = sentence.strip()\n",
    "    return sentence\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6eebf51c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12시 땡 ! 하루가 또 가네요 .', '1지망 학교 떨어졌어 위로해 드립니다 .', '3박4일 놀러가고 싶다 여행은 언제나 좋죠 .', '3박4일 정도 놀러가고 싶다 여행은 언제나 좋죠 .', '심하네 눈살이 찌푸려지죠 .']\n"
     ]
    }
   ],
   "source": [
    "data = list(map(preprocess_sentence, data))\n",
    "print(data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ee071f",
   "metadata": {},
   "source": [
    "모델의 입력으로 사용할 수 있도록 데이터 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cc4570cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시작 토큰과 종료 토큰에 고유한 정수를 부여합니다\n",
    "# 질문과 답변 데이터셋에 대해서 Vocabulary 생성\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(\n",
    "    data, target_vocab_size=2**13)\n",
    "\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dc009931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8351\n"
     ]
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰을 고려하여 +2를 하여 단어장의 크기를 산정합니다.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a60720a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정수 인코딩 후의 21번째 샘플: [5861, 601, 2483, 4206, 8125, 2350, 7668, 8, 6371, 97, 1]\n"
     ]
    }
   ],
   "source": [
    "print('정수 인코딩 후의 21번째 샘플: {}'.format(tokenizer.encode(data[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1741d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2476cdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_filter(sentences):\n",
    "    tokenized = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence = START_TOKEN + tokenizer.encode(sentence) + END_TOKEN\n",
    "\n",
    "        if len(sentence) <= MAX_LENGTH:\n",
    "            tokenized.append(sentence)\n",
    "        \n",
    "    tokenized = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        tokenized, maxlen = MAX_LENGTH, padding = 'post'\n",
    "    )\n",
    "\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "366bebb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어장의 크기 : 8351\n",
      "필터링 후의 질문 샘플 개수: 11823\n"
     ]
    }
   ],
   "source": [
    "data = tokenize_and_filter(data)\n",
    "print('단어장의 크기 :',(VOCAB_SIZE))\n",
    "print('필터링 후의 질문 샘플 개수: {}'.format(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "af5b3cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# 디코더는 이전의 target을 다음의 input으로 사용합니다.\n",
    "# 이에 따라 outputs에서는 START_TOKEN을 제거하겠습니다.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': data[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': data[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15afa929",
   "metadata": {},
   "source": [
    "트랜스포머 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "690143e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포지셔널 인코딩 레이어\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model) # position: max_position\n",
    "\n",
    "    def get_angles(self, position, i, d_model):\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles\n",
    "    \n",
    "    def positional_encoding(self, position, d_model):\n",
    "        # 각도 배열 생성\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "        \n",
    "        # 배열의 짝수 인덱스에는 sin 함수 적용\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "        # 배열의 홀수 인덱스에는 cosine 함수 적용\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        # sin과 cosine이 교차되도록 재배열\n",
    "        pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "        pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "        pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89894e5a",
   "metadata": {},
   "source": [
    "어텐션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f53ea22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 스케일드 닷 프로덕트 어텐션 함수\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "    # 어텐션 가중치는 Q와 K의 닷 프로덕트\n",
    "    matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "    # 가중치를 정규화\n",
    "    depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "    logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "    # 패딩에 마스크 추가\n",
    "    if mask is not None:\n",
    "        logits += (mask * -1e9)\n",
    "\n",
    "    # softmax적용\n",
    "    attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "    # 최종 어텐션은 가중치와 V의 닷 프로덕트\n",
    "    output = tf.matmul(attention_weights, value)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b530e2",
   "metadata": {},
   "source": [
    "멀티 헤드 어텐션"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51549bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "        super(MultiHeadAttention, self).__init__(name=name)\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "        self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    def split_heads(self, inputs, batch_size):\n",
    "        inputs = tf.reshape(\n",
    "            inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, inputs):\n",
    "        query, key, value, mask = inputs['query'], inputs['key'], inputs['value'], inputs['mask']\n",
    "        batch_size = tf.shape(query)[0]\n",
    "\n",
    "        # Q, K, V에 각각 Dense를 적용합니다\n",
    "        query = self.query_dense(query)\n",
    "        key = self.key_dense(key)\n",
    "        value = self.value_dense(value)\n",
    "\n",
    "        # 병렬 연산을 위한 머리를 여러 개 만듭니다\n",
    "        query = self.split_heads(query, batch_size)\n",
    "        key = self.split_heads(key, batch_size)\n",
    "        value = self.split_heads(value, batch_size)\n",
    "\n",
    "        # 스케일드 닷 프로덕트 어텐션 함수\n",
    "        scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        # 어텐션 연산 후에 각 결과를 다시 연결(concatenate)합니다\n",
    "        concat_attention = tf.reshape(scaled_attention,\n",
    "                                          (batch_size, -1, self.d_model))\n",
    "\n",
    "        # 최종 결과에도 Dense를 한 번 더 적용합니다\n",
    "        outputs = self.dense(concat_attention)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fde930",
   "metadata": {},
   "source": [
    "마스킹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dff033f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#패딩 마스킹\n",
    "def create_padding_mask(x):\n",
    "    mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "    # (batch_size, 1, 1, sequence length)\n",
    "    return mask[:, tf.newaxis, tf.newaxis, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7e3152b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 룩 어헤드 마스킹(Look-ahead masking, 다음 단어 가리기) \n",
    "\n",
    "# 트랜스포머의 경우에는 문장 행렬을 만들어 한 번에 행렬 형태로 입력으로 들어간다는 특징이 있습니다. 그리고 이 특징 때문에 추가적인 마스킹(Masking) 을 필요\n",
    "# 전체 문장이 문자 행렬로 들어가서 예측 가능하지만 시도하고자 하는 것은 이전 단어로부터 다음 단어 예측하는 훈련하는것\n",
    "# 자신보다 다음 나올 단어를 참고하지 않도록 가리는 기법이 룩 어헤드 마스킹 기법\n",
    "def create_look_ahead_mask(x):\n",
    "    seq_len = tf.shape(x)[1]\n",
    "    look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "    padding_mask = create_padding_mask(x)\n",
    "    return tf.maximum(look_ahead_mask, padding_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e50a391b",
   "metadata": {},
   "source": [
    "디코더 층\n",
    "encoder-decoder atteion, enc_outputs, padding_mask 제외.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "19661ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 하나의 레이어를 함수로 구현.\n",
    "# 이 하나의 레이어 안에는 세 개의 서브 레이어가 존재합니다.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "    inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "    # enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "    # padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "    # 첫 번째 서브 레이어 : 멀티 헤드 어텐션 수행 (셀프 어텐션)\n",
    "    attention1 = MultiHeadAttention(\n",
    "        d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "            'query': inputs,\n",
    "            'key': inputs,\n",
    "            'value': inputs,\n",
    "            'mask': look_ahead_mask\n",
    "        })\n",
    "\n",
    "    # 멀티 헤드 어텐션의 결과는 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    attention1 = tf.keras.layers.Dropout(rate=dropout)(attention1)\n",
    "    attention1 = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "    # # 두 번째 서브 레이어 : 마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)\n",
    "    # attention2 = MultiHeadAttention(\n",
    "    #     d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "    #         'query': attention1,\n",
    "    #         'key': enc_outputs,\n",
    "    #         'value': enc_outputs,\n",
    "    #         'mask': padding_mask\n",
    "    #     })\n",
    "\n",
    "    # # 마스크드 멀티 헤드 어텐션의 결과는\n",
    "    # # Dropout과 LayerNormalization이라는 훈련을 돕는 테크닉을 수행\n",
    "    # attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "    # attention2 = tf.keras.layers.LayerNormalization(\n",
    "    #     epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "    #마스크드 멀티 헤드 어텐션 수행 (인코더-디코더 어텐션)제외\n",
    "    \n",
    "\n",
    "    # 세 번째 서브 레이어 : 2개의 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention1)\n",
    "    outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "    # 완전연결층의 결과는 Dropout과 LayerNormalization 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "    outputs = tf.keras.layers.LayerNormalization(\n",
    "        epsilon=1e-6)(outputs + attention1)\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, look_ahead_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f3fd7b",
   "metadata": {},
   "source": [
    "디코더\n",
    "enc_outputs, padding_mask 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b4f9ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "    inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "    # enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "    look_ahead_mask = tf.keras.Input(\n",
    "        shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "    # 패딩 마스크\n",
    "    # padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "    # 임베딩 레이어\n",
    "    embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "    embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "    # 포지셔널 인코딩\n",
    "    embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "    # Dropout이라는 훈련을 돕는 테크닉을 수행\n",
    "    outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "    for i in range(num_layers):\n",
    "        outputs = decoder_layer(\n",
    "            units=units,\n",
    "            d_model=d_model,\n",
    "            num_heads=num_heads,\n",
    "            dropout=dropout,\n",
    "            name='decoder_layer_{}'.format(i),\n",
    "        )(inputs=[outputs, look_ahead_mask])\n",
    "\n",
    "    return tf.keras.Model(\n",
    "        inputs=[inputs, look_ahead_mask],\n",
    "        outputs=outputs,\n",
    "        name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7152468f",
   "metadata": {},
   "source": [
    "모델 정의 및 학습하기\n",
    "트랜스포머 함수 정의\n",
    "\n",
    "인코더 안쓰니까 제외, 디코더 두번째 마스킼 안쓰니까 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c1550831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "    inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "    # dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "    # # 인코더에서 패딩을 위한 마스크\n",
    "    # enc_padding_mask = tf.keras.layers.Lambda(\n",
    "    #     create_padding_mask, output_shape=(1, 1, None),\n",
    "    #     name='enc_padding_mask')(inputs)\n",
    "\n",
    "    # 디코더에서 미래의 토큰을 마스크 하기 위해서 사용합니다.\n",
    "    # 내부적으로 패딩 마스크도 포함되어져 있습니다.\n",
    "    look_ahead_mask = tf.keras.layers.Lambda(\n",
    "        create_look_ahead_mask,\n",
    "        output_shape=(1, None, None),\n",
    "        name='look_ahead_mask')(inputs)\n",
    "\n",
    "    # # 두 번째 어텐션 블록에서 인코더의 벡터들을 마스킹\n",
    "    # # 디코더에서 패딩을 위한 마스크\n",
    "    # dec_padding_mask = tf.keras.layers.Lambda(\n",
    "    #     create_padding_mask, output_shape=(1, 1, None),\n",
    "    #     name='dec_padding_mask')(inputs)\n",
    "\n",
    "    # # 인코더\n",
    "    # enc_outputs = encoder(\n",
    "    #     vocab_size=vocab_size,\n",
    "    #     num_layers=num_layers,\n",
    "    #     units=units,\n",
    "    #     d_model=d_model,\n",
    "    #     num_heads=num_heads,\n",
    "    #     dropout=dropout,\n",
    "    # )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "    # 디코더\n",
    "    dec_outputs = decoder(\n",
    "        vocab_size=vocab_size,\n",
    "        num_layers=num_layers,\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "    )(inputs=[inputs, look_ahead_mask])\n",
    "\n",
    "    # 완전연결층\n",
    "    outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e1957f",
   "metadata": {},
   "source": [
    "모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9f88172b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inputs (InputLayer)             [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "look_ahead_mask (Lambda)        (None, 1, None, None 0           inputs[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Functional)            (None, None, 256)    3192064     inputs[0][0]                     \n",
      "                                                                 look_ahead_mask[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "outputs (Dense)                 (None, None, 8351)   2146207     decoder[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,338,271\n",
      "Trainable params: 5,338,271\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# 하이퍼파라미터\n",
    "NUM_LAYERS = 2 # 인코더와 디코더의 층의 개수\n",
    "D_MODEL = 256 # 인코더와 디코더 내부의 입, 출력의 고정 차원\n",
    "NUM_HEADS = 8 # 멀티 헤드 어텐션에서의 헤드 수 \n",
    "UNITS = 512 # 피드 포워드 신경망의 은닉층의 크기\n",
    "DROPOUT = 0.1 # 드롭아웃의 비율\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee45daf8",
   "metadata": {},
   "source": [
    "손실 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5f2c02b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "    mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "    loss = tf.multiply(loss, mask)\n",
    "\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeb0489",
   "metadata": {},
   "source": [
    "커스템된 학습률"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a78084f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb438fc",
   "metadata": {},
   "source": [
    "모델 컴파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "da22d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "    y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "    return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b95e22f",
   "metadata": {},
   "source": [
    "훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0fb6cc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "185/185 [==============================] - 26s 41ms/step - loss: 2.1802 - accuracy: 0.0205\n",
      "Epoch 2/20\n",
      "185/185 [==============================] - 8s 41ms/step - loss: 1.9003 - accuracy: 0.0394\n",
      "Epoch 3/20\n",
      "185/185 [==============================] - 8s 41ms/step - loss: 1.7177 - accuracy: 0.0436\n",
      "Epoch 4/20\n",
      "185/185 [==============================] - 8s 41ms/step - loss: 1.6219 - accuracy: 0.0489\n",
      "Epoch 5/20\n",
      "185/185 [==============================] - 8s 42ms/step - loss: 1.5337 - accuracy: 0.0542\n",
      "Epoch 6/20\n",
      "185/185 [==============================] - 8s 42ms/step - loss: 1.4478 - accuracy: 0.0585\n",
      "Epoch 7/20\n",
      "185/185 [==============================] - 8s 42ms/step - loss: 1.3643 - accuracy: 0.0629\n",
      "Epoch 8/20\n",
      "185/185 [==============================] - 8s 41ms/step - loss: 1.2800 - accuracy: 0.0687\n",
      "Epoch 9/20\n",
      "185/185 [==============================] - 8s 42ms/step - loss: 1.1910 - accuracy: 0.0754\n",
      "Epoch 10/20\n",
      "185/185 [==============================] - 8s 42ms/step - loss: 1.0997 - accuracy: 0.0823\n",
      "Epoch 11/20\n",
      "185/185 [==============================] - 8s 41ms/step - loss: 1.0084 - accuracy: 0.0895\n",
      "Epoch 12/20\n",
      "185/185 [==============================] - 8s 41ms/step - loss: 0.9176 - accuracy: 0.0977\n",
      "Epoch 13/20\n",
      "185/185 [==============================] - 8s 41ms/step - loss: 0.8294 - accuracy: 0.1070\n",
      "Epoch 14/20\n",
      "185/185 [==============================] - 8s 41ms/step - loss: 0.7442 - accuracy: 0.1175\n",
      "Epoch 15/20\n",
      "185/185 [==============================] - 8s 41ms/step - loss: 0.6637 - accuracy: 0.1286\n",
      "Epoch 16/20\n",
      "185/185 [==============================] - 8s 41ms/step - loss: 0.5895 - accuracy: 0.1404\n",
      "Epoch 17/20\n",
      "185/185 [==============================] - 8s 41ms/step - loss: 0.5246 - accuracy: 0.1515\n",
      "Epoch 18/20\n",
      "185/185 [==============================] - 8s 41ms/step - loss: 0.4706 - accuracy: 0.1617\n",
      "Epoch 19/20\n",
      "185/185 [==============================] - 8s 41ms/step - loss: 0.4258 - accuracy: 0.1705\n",
      "Epoch 20/20\n",
      "185/185 [==============================] - 8s 41ms/step - loss: 0.3928 - accuracy: 0.1773\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=3)\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, verbose=1, callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f1fb45",
   "metadata": {},
   "source": [
    "모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a19207dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # 입력된 문장을 정수 인코딩 후, 시작 토큰과 종료 토큰을 앞뒤로 추가.\n",
    "    # ex) Where have you been? → [[8331   86   30    5 1059    7 8332]]\n",
    "    sentence = tf.expand_dims(\n",
    "        START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "    # 디코더의 현재까지의 예측한 출력 시퀀스가 지속적으로 저장되는 변수.\n",
    "    # 처음에는 예측한 내용이 없음으로 시작 토큰만 별도 저장. ex) 8331\n",
    "    output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "    # 디코더의 인퍼런스 단계\n",
    "    for i in range(MAX_LENGTH):\n",
    "        # 디코더는 최대 MAX_LENGTH의 길이만큼 다음 단어 예측을 반복합니다.\n",
    "        # predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "        predictions = model(inputs=sentence, training=False)\n",
    "        predictions = predictions[:, -1:, :]\n",
    "\n",
    "        # 현재 예측한 단어의 정수\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "        # 만약 현재 예측한 단어가 종료 토큰이라면 for문을 종료\n",
    "        if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "            break\n",
    "\n",
    "        # 예측한 단어들은 지속적으로 output_sequence에 추가됩니다.\n",
    "        # 이 output_sequence는 다시 디코더의 입력이 됩니다.\n",
    "        output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output_sequence, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "367f73c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "    # 입력 문장에 대해서 디코더를 동작 시켜 예측된 정수 시퀀스를 리턴받습니다.\n",
    "    prediction = decoder_inference(sentence)\n",
    "\n",
    "    # 정수 시퀀스를 다시 텍스트 시퀀스로 변환합니다.\n",
    "    predicted_sentence = tokenizer.decode(\n",
    "        [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "    print('입력 : {}'.format(sentence))\n",
    "    print('출력 : {}'.format(predicted_sentence))\n",
    "\n",
    "    return predicted_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1c839870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 오늘 날씨 어때?\n",
      "출력 : 깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨깨'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('오늘 날씨 어때?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9d78ffa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 깨방정하구나\n",
      "출력 : 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 수 '"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('깨방정하구나')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ab06fbf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 그래 끝말잇기하자 수박\n",
      "출력 : 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 로 '"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('그래 끝말잇기하자 수박')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5d1b864a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 로미오와 줄리엣\n",
      "출력 :                                                   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'                                                  '"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('로미오와 줄리엣')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0fd1c71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : 왜그래? 고장났니?\n",
      "출력 : 두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두두'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('왜그래? 고장났니?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "94466f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 : ㅠㅠ\n",
      "출력 : 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 하면 '"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('ㅠㅠ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83777fc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
