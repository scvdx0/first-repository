{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49d89fbb",
   "metadata": {},
   "source": [
    "# Step 1. ë°ì´í„° ìˆ˜ì§‘í•˜ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809a0348",
   "metadata": {},
   "source": [
    "íŒ¨í‚¤ì§€ ì„í¬íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446d5d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0dfae0",
   "metadata": {},
   "source": [
    "### ë°ì´í„° ë¡œë”©: CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  í•„ìš”í•œ ë°ì´í„°ë¡œ ë³€í™˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c716b36",
   "metadata": {},
   "source": [
    "ë°ì´í„° ë¡œë”©: ChatbotData.csv íŒŒì¼ì„ ë¡œë“œí•˜ê³  íŒŒì‹±í•˜ëŠ” ì½”ë“œë¥¼ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "í”„ë¡œì íŠ¸ì˜ ê¸°ì¡´ ë°ì´í„° ë¡œë”© ì½”ë“œëŠ” ì˜í™” ëŒ€í™” ë°ì´í„°ì…‹ì„ ë¡œë“œí•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ë¥¼ í•œêµ­ì–´ ì±—ë´‡ ë°ì´í„°ë¡œ ëŒ€ì²´í•´ì•¼ í•©ë‹ˆë‹¤.\n",
    "CSV íŒŒì¼ì„ ì½ì–´ë“¤ì´ê³  ì ì ˆí•œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì‘ì—…ì´ í•„ìš”í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21b765",
   "metadata": {},
   "source": [
    "# Step 2. ë°ì´í„° ì „ì²˜ë¦¬í•˜ê¸°\n",
    "ì „ì²˜ë¦¬ í•¨ìˆ˜: ë¬¸ì¥ì—ì„œ í•„ìš” ì—†ëŠ” ë¬¸ì ì œê±° ë° êµ¬ë‘ì  ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae2c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²˜ë¦¬ í•¨ìˆ˜\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.strip()\n",
    "    # êµ¬ë‘ì ê³¼ í•œê¸€ ì‚¬ì´ì— ê³µë°± ì¶”ê°€\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    # í•œê¸€ê³¼ êµ¬ë‘ì ì„ ì œì™¸í•œ ëª¨ë“  ë¬¸ìë¥¼ ê³µë°±ìœ¼ë¡œ ëŒ€ì²´\n",
    "    sentence = re.sub(r\"[^ê°€-í£?0-9?.!,]+\", \" \", sentence) #í—ˆìš©í•  ë¶€ë¶„, ë‚˜ë¨¸ì§€ ê³µë°±\n",
    "    return sentence\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac08ab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_conversations():\n",
    "    data_path = 'data/ChatbotData.csv'\n",
    "    data = pd.read_csv(data_path)\n",
    "    inputs, outputs = [], []\n",
    "    for _, row in data.iterrows():\n",
    "      inputs.append(preprocess_sentence(row['Q']))\n",
    "      outputs.append(preprocess_sentence(row['A']))\n",
    "    return inputs, outputs\n",
    "\n",
    "load_conversations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eefe752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_df = pd.read_csv(\"data/ChatbotData.csv\")\n",
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669b05d2",
   "metadata": {},
   "source": [
    "# Step 4. íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ êµ¬ì„±\n",
    "# í¬ì§€ì…”ë„ ì¸ì½”ë”© ë ˆì´ì–´: ê° ë‹¨ì–´ì˜ ìœ„ì¹˜ì— ë”°ë¼ ë‹¤ë¥¸ ì‹ í˜¸ë¥¼ ëª¨ë¸ì— ì œê³µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72040b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í¬ì§€ì…”ë„ ì¸ì½”ë”© ë ˆì´ì–´ ìƒì„± => ë‹¨ì–´ë¥¼ ì¸ì½”ë”©ì„ í•˜ê³  ìœ„ì¹˜ê°’ì„ ë”í•´ì£¼ëŠ”ê²ƒ \n",
    "# ì„ë² ë”© í–‰ë ¬ê³¼ í¬ì§€ì…”ë„ í–‰ë ¬ì´ë¼ëŠ” ë‘ í–‰ë ¬ì„ ë”í•¨ìœ¼ë¡œì¨ ê° ë‹¨ì–´ ë²¡í„°ì— ìœ„ì¹˜ ì •ë³´ë¥¼ ë”í•´ì£¼ê²Œ ë˜ëŠ” ê²ƒ\n",
    "# ê°™ì€ ë‹¨ì–´ë¼ê³  í•˜ë”ë¼ë„ í¬ì§€ì…”ë„ ì¸ì½”ë”©ì„ í•´ì¤€ ê²½ìš°ì—ëŠ” ì„ë² ë”© ë²¡í„°ê°’ì´ ë‹¬ë¼ì§€ë¯€ë¡œ, ê°™ì€ ë‹¨ì–´ë¼ê³  í•´ë„ \n",
    "# ê°ê° ë‹¤ë¥¸ ìœ„ì¹˜ì— ë“±ì¥í–ˆë‹¤ëŠ” ì‚¬ì‹¤ì„ ëª¨ë¸ì— ì•Œë ¤ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, position, d_model):\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "\n",
    "  def get_angles(self, position, i, d_model):\n",
    "    angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "    return position * angles\n",
    "\n",
    "  def positional_encoding(self, position, d_model):\n",
    "    # ê°ë„ ë°°ì—´ ìƒì„±\n",
    "    angle_rads = self.get_angles(\n",
    "        position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "        i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "        d_model=d_model)\n",
    "\n",
    "    # ë°°ì—´ì˜ ì§ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” sin í•¨ìˆ˜ ì ìš©\n",
    "    sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "    # ë°°ì—´ì˜ í™€ìˆ˜ ì¸ë±ìŠ¤ì—ëŠ” cosine í•¨ìˆ˜ ì ìš©\n",
    "    cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    # sinê³¼ cosineì´ êµì°¨ë˜ë„ë¡ ì¬ë°°ì—´\n",
    "    pos_encoding = tf.stack([sines, cosines], axis=0)\n",
    "    pos_encoding = tf.transpose(pos_encoding,[1, 2, 0]) \n",
    "    pos_encoding = tf.reshape(pos_encoding, [position, d_model])\n",
    "\n",
    "    pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, tf.float32)\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b79526",
   "metadata": {},
   "outputs": [],
   "source": [
    "#í–‰ì˜ í¬ê¸°ê°€ 50, ì—´ì˜ í¬ê¸°ê°€ 512ì¸ í–‰ë ¬ì„ ê·¸ë ¤ë´…ì‹œë‹¤. \n",
    "#ì´ë¥¼í…Œë©´, ìµœëŒ€ ë¬¸ì¥ì˜ ê¸¸ì´ê°€ 50ì´ê³  ì›Œë“œ ì„ë² ë”© ì°¨ì›ì„ 512ë¡œ í•˜ëŠ” ëª¨ë¸ì˜ ì…ë ¥ ë²¡í„° ëª¨ì–‘ì´ ì´ì™€ ê°™ì„ ê²ƒì…ë‹ˆë‹¤.\n",
    "sample_pos_encoding = PositionalEncoding(50, 512) # Assuming PositionalEncoding is defined correctly\n",
    "\n",
    "plt.pcolormesh(sample_pos_encoding.pos_encoding.numpy()[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12053d14",
   "metadata": {},
   "source": [
    "# ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜ ë° ë©€í‹° í—¤ë“œ ì–´í…ì…˜ êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c95e9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜ í•¨ìˆ˜ => ë‹¨ì–´ ë²¡í„°ê°„ ìœ ì‚¬ë„ì¸ë° ì†Œí”„íŠ¸ë§¥ìŠ¤ë¥¼ ì ìš©\n",
    "# Attention(Q,K,V)=softmax(QKTâˆšdk)Vğ´ğ‘¡ğ‘¡ğ‘’ğ‘›ğ‘¡ğ‘–ğ‘œğ‘›(ğ‘„,ğ¾,ğ‘‰)=ğ‘ ğ‘œğ‘“ğ‘¡ğ‘šğ‘ğ‘¥(ğ‘„ğ¾ğ‘‡ğ‘‘ğ‘˜)ğ‘‰\n",
    "# ë‚´ì (dot product)ì„ í†µí•´ \"ë‹¨ì–´ ë²¡í„° ê°„ ìœ ì‚¬ë„ë¥¼ êµ¬í•œ í›„ì—\" \n",
    "# íŠ¹ì • ê°’ì„ ë¶„ëª¨ë¡œ ë‚˜ëˆ ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œÂ Qì™€Â Kì˜ ìœ ì‚¬ë„ë¥¼ êµ¬í•˜ì˜€ë‹¤ê³  í•˜ì—¬\n",
    "# ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜(Scaled Dot Product Attention)Â ì´ë¼ê³  í•©ë‹ˆë‹¤.\n",
    "\n",
    "def scaled_dot_product_attention(query, key, value, mask):\n",
    "  # ì–´í…ì…˜ ê°€ì¤‘ì¹˜ëŠ” Qì™€ Kì˜ ë‹· í”„ë¡œë•íŠ¸\n",
    "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
    "\n",
    "  # ê°€ì¤‘ì¹˜ë¥¼ ì •ê·œí™”\n",
    "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
    "  logits = matmul_qk / tf.math.sqrt(depth)\n",
    "\n",
    "  # íŒ¨ë”©ì— ë§ˆìŠ¤í¬ ì¶”ê°€\n",
    "  if mask is not None:\n",
    "    logits += (mask * -1e9)\n",
    "\n",
    "  # softmaxì ìš©\n",
    "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
    "\n",
    "  # ìµœì¢… ì–´í…ì…˜ì€ ê°€ì¤‘ì¹˜ì™€ Vì˜ ë‹· í”„ë¡œë•íŠ¸\n",
    "  output = tf.matmul(attention_weights, value)\n",
    "  return output\n",
    "\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì–´í…ì…˜ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ëŠ” ê²ƒì„Â ë©€í‹° í—¤ë“œ ì–´í…ì…˜ => ë¶„ì‚°ì²˜ë¦¬\n",
    "#ë‚´ë¶€ì ìœ¼ë¡œëŠ”Â ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ëŠ” í˜•íƒœë¡œ ë˜ì–´ìˆìŒ\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "\n",
    "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
    "    super(MultiHeadAttention, self).__init__(name=name)\n",
    "    self.num_heads = num_heads\n",
    "    self.d_model = d_model\n",
    "\n",
    "    assert d_model % self.num_heads == 0\n",
    "\n",
    "    self.depth = d_model // self.num_heads\n",
    "\n",
    "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
    "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
    "\n",
    "  def split_heads(self, inputs, batch_size):\n",
    "    inputs = tf.reshape(\n",
    "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
    "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    query, key, value, mask = inputs['query'], inputs['key'], inputs[\n",
    "        'value'], inputs['mask']\n",
    "    batch_size = tf.shape(query)[0]\n",
    "\n",
    "    # Q, K, Vì— ê°ê° Denseë¥¼ ì ìš©í•©ë‹ˆë‹¤\n",
    "    query = self.query_dense(query)  # [[YOUR CODE]]\n",
    "    key = self.key_dense(key)  # [[YOUR CODE]]\n",
    "    value = self.value_dense(value)  # [[YOUR CODE]]\n",
    "\n",
    "    # ë³‘ë ¬ ì—°ì‚°ì„ ìœ„í•œ ë¨¸ë¦¬ë¥¼ ì—¬ëŸ¬ ê°œ ë§Œë“­ë‹ˆë‹¤\n",
    "    query = self.split_heads(query, batch_size)  # [[YOUR CODE]]\n",
    "    key = self.split_heads(key, batch_size)  # [[YOUR CODE]]\n",
    "    value = self.split_heads(value, batch_size)  # [[YOUR CODE]]\n",
    "\n",
    "    # ìŠ¤ì¼€ì¼ë“œ ë‹· í”„ë¡œë•íŠ¸ ì–´í…ì…˜ í•¨ìˆ˜\n",
    "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
    "\n",
    "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "    # ì–´í…ì…˜ ì—°ì‚° í›„ì— ê° ê²°ê³¼ë¥¼ ë‹¤ì‹œ ì—°ê²°(concatenate)í•©ë‹ˆë‹¤\n",
    "    concat_attention = tf.reshape(scaled_attention,\n",
    "                                  (batch_size, -1, self.d_model))\n",
    "\n",
    "    # ìµœì¢… ê²°ê³¼ì—ë„ Denseë¥¼ í•œ ë²ˆ ë” ì ìš©í•©ë‹ˆë‹¤\n",
    "    outputs = self.dense(concat_attention)\n",
    "\n",
    "    return outputs\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5655b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#íŒ¨ë”© ë§ˆìŠ¤í‚¹ ì‘ìš©, íŒ¨ë”©ì€ ë¬¸ì¥ ê¸¸ì´ë¥¼ ë§ì¶°ì£¼ëŠ” ì—­í™œì´ê³   íŒ¨ë”© ë§ˆìŠ¤í‚¹ì€ ì´ë¥¼ ìœ„í•´ ìˆ«ì 0ì¸ ìœ„ì¹˜ë¥¼ ì²´í¬í•©ë‹ˆë‹¤.\n",
    "def create_padding_mask(x):\n",
    "  mask = tf.cast(tf.math.equal(x, 0), tf.float32)\n",
    "  # (batch_size, 1, 1, sequence length)\n",
    "  return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "print(\"ìŠ=3\")\n",
    "#ë°ì´í„°ê°€ ìˆëŠ” ê³³ì€ 0 ë¹ˆê³³ì€ 1ë¡œ ì±„ì›Œì§„ ê²ƒì„ í™•ì¸\n",
    "print(create_padding_mask(tf.constant([[1, 2, 0, 3, 0], [0, 0, 0, 4, 5]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021ec831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë£© ì–´í—¤ë“œ ë§ˆìŠ¤í‚¹(Look-ahead masking, ë‹¤ìŒ ë‹¨ì–´ ê°€ë¦¬ê¸°) \n",
    "# íŠ¸ëœìŠ¤í¬ë¨¸ì˜ ê²½ìš°ì—ëŠ” ë¬¸ì¥ í–‰ë ¬ì„ ë§Œë“¤ì–´ í•œ ë²ˆì— í–‰ë ¬ í˜•íƒœë¡œ ì…ë ¥ìœ¼ë¡œ ë“¤ì–´ê°„ë‹¤ëŠ” íŠ¹ì§•ì´ ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ íŠ¹ì§• ë•Œë¬¸ì— ì¶”ê°€ì ì¸ ë§ˆìŠ¤í‚¹(Masking) ì„ í•„ìš”\n",
    "# ì „ì²´ ë¬¸ì¥ì´ ë¬¸ì í–‰ë ¬ë¡œ ë“¤ì–´ê°€ì„œ ì˜ˆì¸¡ ê°€ëŠ¥í•˜ì§€ë§Œ ì‹œë„í•˜ê³ ì í•˜ëŠ” ê²ƒì€ ì´ì „ ë‹¨ì–´ë¡œë¶€í„° ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡í•˜ëŠ” í›ˆë ¨í•˜ëŠ”ê²ƒ\n",
    "# ìì‹ ë³´ë‹¤ ë‹¤ìŒ ë‚˜ì˜¬ ë‹¨ì–´ë¥¼ ì°¸ê³ í•˜ì§€ ì•Šë„ë¡ ê°€ë¦¬ëŠ” ê¸°ë²•ì´ ë£© ì–´í—¤ë“œ ë§ˆìŠ¤í‚¹ ê¸°ë²•\n",
    "\n",
    "def create_look_ahead_mask(x):\n",
    "  seq_len = tf.shape(x)[1]\n",
    "  look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "  padding_mask = create_padding_mask(x)\n",
    "  return tf.maximum(look_ahead_mask, padding_mask)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9becc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸\n",
    "print(create_look_ahead_mask(tf.constant([[1, 2, 3, 4, 5]])))\n",
    "#ìˆ«ì 0 ë„£ì–´ í…ŒìŠ¤íŠ¸ \n",
    "print(create_look_ahead_mask(tf.constant([[0, 5, 1, 5, 5]])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1e70f0",
   "metadata": {},
   "source": [
    "ë²ˆì—­ê¸°ëŠ” ì¸ì½”ë”ì™€ ë””ì½”ë” ë‘ ê°€ì§€ ì•„í‚¤í…ì²˜ë¡œ êµ¬ì„±ë¼ ìˆì—ˆìŠµë‹ˆë‹¤. ì¸ì½”ë”ì— ì…ë ¥ ë¬¸ì¥ì´ ë“¤ì–´ê°€ê³ , ë””ì½”ë”ëŠ” ì´ì— ìƒì‘í•˜ëŠ” ì¶œë ¥ ë¬¸ì¥ì„ ìƒì„±í•©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ë¥¼ í›ˆë ¨í•œë‹¤ëŠ” ê²ƒì€ ê²°êµ­ ì…ë ¥ ë¬¸ì¥ê³¼ ì¶œë ¥ ë¬¸ì¥ ë‘ ê°€ì§€ ë³‘ë ¬ êµ¬ì¡°ë¡œ êµ¬ì„±ëœ ë°ì´í„°ì…‹ì„ í›ˆë ¨í•œë‹¤ëŠ” ì˜ë¯¸\n",
    "\n",
    "### í›ˆë ¨ ë°ì´í„°ì…‹ì˜ êµ¬ì„±(ë²ˆì—­)\n",
    "\n",
    "ì…ë ¥ ë¬¸ì¥ : 'ì €ëŠ” í•™ìƒì…ë‹ˆë‹¤.'\n",
    "ì¶œë ¥ ë¬¸ì¥ : 'I am a student'\n",
    "\n",
    "### í›ˆë ¨ ë°ì´í„°ì…‹ì˜ êµ¬ì„±(ì§ˆë¬¸-ë‹µë³€)\n",
    "\n",
    "ì…ë ¥ ë¬¸ì¥ : 'ì˜¤ëŠ˜ì˜ ë‚ ì”¨ëŠ” ì–´ë•Œ?'\n",
    "ì¶œë ¥ ë¬¸ì¥ : 'ì˜¤ëŠ˜ì€ ë§¤ìš° í™”ì°½í•œ ë‚ ì”¨ì•¼\n",
    "\n",
    "ì´ëŸ°ì‹ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µì´ êµ¬ì„±ëœ ë°ì´í„°ì…‹ or ì…ë ¥ëœ ë¬¸ìì— ì‚¬ì‘í•˜ëŠ” ë²ˆì—­ë¬¸ì ë“±ë“±\n",
    "ì–´ë–¤ ë°ì´í„° ì…‹ìœ¼ë¡œ ì¸ì½”ë”ì™€ ë””ì½”ë”ë¥¼ êµ¬ì¡°í•˜ë©´ ì›í•˜ëŠ” ì±—ë´‡ì„ ë§Œë“œëŠ” ê²ƒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¸ì½”ë” ì¸µ ë§Œë“¤ê¸°\n",
    "# ì…€í”„ ì–´í…ì…˜ì€Â ë©€í‹° í—¤ë“œ ì–´í…ì…˜ìœ¼ë¡œ ë³‘ë ¬ì ìœ¼ë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤.\n",
    "# ë‘ ê°œì˜ ì„œë¸Œ ì¸µì„ ê°€ì§€ëŠ” í•˜ë‚˜ì˜ ì¸ì½”ë” ì¸µì„ êµ¬í˜„í•˜ëŠ” í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \n",
    "# í•¨ìˆ˜ ë‚´ë¶€ì ìœ¼ë¡œ ì²« ë²ˆì§¸ ì„œë¸Œ ì¸µê³¼ ë‘ ë²ˆì§¸ ì„œë¸Œ ì¸µì„ êµ¬í˜„\n",
    "\n",
    "def encoder_layer(units, d_model, num_heads, dropout, name=\"encoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "\n",
    "    # íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©\n",
    "    # íŒ¨ë”© ë§ˆìŠ¤í¬ëŠ” ì…ë ¥ ë°ì´í„°ì˜ íŒ¨ë”© í† í°ì„ ë¬´ì‹œí•˜ê±°ë‚˜ \n",
    "    # ë§ˆìŠ¤í‚¹í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. \n",
    "    # íŒ¨ë”© í† í°ì€ ì˜ë¯¸ ìˆëŠ” ì •ë³´ë¥¼ ì „ë‹¬í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ \n",
    "    # ìê¸°ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ê³„ì‚°ì— ì˜í–¥ì„ ì£¼ì–´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤. \n",
    "    # ì´ í† í°ë“¤ì„ ë§ˆìŠ¤í‚¹í•¨ìœ¼ë¡œì¨ ëª¨ë¸ì€ ì‹¤ì œ ë°ì´í„°ì—ë§Œ ì§‘ì¤‘í•˜ì—¬ \n",
    "    # ì£¼ì˜ ê³„ì‚°ì˜ ì§ˆê³¼ ê´€ë ¨ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.\n",
    "\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # ì²« ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì…€í”„ ì–´í…ì…˜)\n",
    "  attention = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention\")({\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "# ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ” Dropoutê³¼ Layer Normalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "\n",
    "# ë“œë¡­ì•„ì›ƒ: ë“œë¡­ì•„ì›ƒì€ ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ê·œì œ ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
    "# í›ˆë ¨ ì¤‘ì— ë“œë¡­ì•„ì›ƒì€ ë¬´ì‘ìœ„ë¡œ ë‰´ëŸ°ì˜ ë¶€ë¶„ ì§‘í•©ì„ ë¬´ì‹œí•˜ì—¬\n",
    "# ë„¤íŠ¸ì›Œí¬ê°€ íŠ¹ì • íŠ¹ì§•ì˜ ë¶€ì¬ì— ê°•ì¸í•˜ê²Œ ë§Œë“¤ê³  ë„¤íŠ¸ì›Œí¬ ë‰´ëŸ° ê°„ì˜ ë…ë¦½ì„±ì„ ì´‰ì§„í•©ë‹ˆë‹¤.\n",
    "# ì´ëŠ” ë³´ì´ì§€ ì•ŠëŠ” ë°ì´í„°ì— ëŒ€í•œ ì¼ë°˜í™”ë¥¼ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "  attention = tf.keras.layers.Dropout(rate=dropout)(attention)\n",
    "# LayerNormalization: ì´ ê¸°ìˆ ì€ ë°°ì¹˜ ì°¨ì›ì´ ì•„ë‹Œ íŠ¹ì§•ì„ í†µí•´ ì…ë ¥ì„ ì •ê·œí™”í•©ë‹ˆë‹¤. \n",
    "# íŠ¸ëœìŠ¤í¬ë¨¸ì™€ ê°™ì€ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œ ë ˆì´ì–´ ì •ê·œí™”ëŠ” í•™ìŠµ ê³¼ì •ì„ ì•ˆì •í™”í•˜ê³ \n",
    "# ìˆ˜ë ´ì— í•„ìš”í•œ í›ˆë ¨ ë‹¨ê³„ ìˆ˜ë¥¼ ì¤„ì—¬ í›ˆë ¨ ì†ë„ë¥¼ ë†’ì´ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.\n",
    "  attention = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(inputs + attention)\n",
    "\n",
    "  # ë‘ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : 2ê°œì˜ ì™„ì „ì—°ê²°ì¸µ\n",
    "# ì™„ì „ ì—°ê²° ê³„ì¸µ: ì£¼ì˜ ë©”ì»¤ë‹ˆì¦˜ ë‹¤ìŒì— ê° ì¸ì½”ë”ì—ì„œ ì™„ì „ ì—°ê²° ê³„ì¸µ\n",
    "# (ë˜ëŠ” í”¼ë“œí¬ì›Œë“œ ë„¤íŠ¸ì›Œí¬)ì€ ì£¼ì˜ ì¶œë ¥ íŠ¹ì§• ê³µê°„ì„ \n",
    "# ë” ë†’ì€ ìˆ˜ì¤€ì˜ ì¶”ìƒí™”ë¥¼ í¬ì°©í•˜ëŠ” í‘œí˜„ìœ¼ë¡œë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
    "# ì´ ê³„ì¸µì€ ëª¨ë¸ì— ë¹„ì„ í˜• ëŠ¥ë ¥ì„ ë„ì…í•  ìˆ˜ë„ ìˆì–´ ë” ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.\n",
    "\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # ì™„ì „ì—°ê²°ì¸µì˜ ê²°ê³¼ëŠ” Dropoutê³¼ LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention + outputs)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38b77f0",
   "metadata": {},
   "source": [
    "ë“œë¡­ì•„ì›ƒê³¼ ë ˆì´ì–´ ì •ê·œí™”ì™€ ê°™ì€ ê¸°ìˆ ì€ ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ëª¨ë¸ì˜ í›ˆë ¨ì„ ì•ˆì •í™”í•˜ëŠ” ë° ë„ì›€ì„ ì£¼ëŠ” ë™ì‹œì— ì „ë°˜ì ì¸ í•™ìŠµ ì—­í•™ì„ ê°œì„ í•©ë‹ˆë‹¤. ì´ëŠ” ì–¸ì–´ ì´í•´ì™€ ìƒì„±ê³¼ ê°™ì€ ì‘ì—…ì—ì„œ ë§¥ë½ê³¼ ë¯¸ë¬˜í•œ ì–¸ì–´ì  íŠ¹ì§•ì´ ì¤‘ìš”í•œ ì—­í• ì„ í•˜ëŠ” ë° ìˆì–´ ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•˜ëŠ” ë° í•„ìˆ˜ì ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ëŸ¬í•œ êµ¬ì„± ìš”ì†Œë¥¼ ì¸ì½”ë” ë ˆì´ì–´ì— í†µí•©í•¨ìœ¼ë¡œì¨ íŠ¸ëœìŠ¤í¬ë¨¸ ì•„í‚¤í…ì²˜ëŠ” ì‹œí€€ìŠ¤ ê°„ ì‘ì—…ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ ì‘ìš© í”„ë¡œê·¸ë¨ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ì´ êµ¬ì„± ìš”ì†Œë“¤ì€ ëª¨ë¸ì´ ë°ì´í„°ì˜ íš¨ê³¼ì ì¸ í‘œí˜„ì„ í•™ìŠµí•˜ë„ë¡ ë³´ì¥í•˜ë©°, ê° ë ˆì´ì–´ëŠ” ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ë” ê¹Šì€ ì´í•´ì™€ ì²˜ë¦¬ì— ê¸°ì—¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4017bad9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#ì¸ì½”ë” ì¸µì„ ìŒ“ì•„ ì¸ì½”ë” ë§Œë“¤ê¸°\n",
    "\n",
    "def encoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name=\"encoder\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "\n",
    "  # íŒ¨ë”© ë§ˆìŠ¤í¬ ì‚¬ìš©\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name=\"padding_mask\")\n",
    "\n",
    "  # ì„ë² ë”© ë ˆì´ì–´\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # í¬ì§€ì…”ë„ ì¸ì½”ë”© \n",
    "  # ì„ë² ë”© í–‰ë ¬ê³¼ í¬ì§€ì…”ë„ í–‰ë ¬ì´ë¼ëŠ” ë‘ í–‰ë ¬ì„ ë”í•¨ìœ¼ë¡œì¨ ê° ë‹¨ì–´ ë²¡í„°ì— ìœ„ì¹˜ ì •ë³´ë¥¼ ë”í•´ì£¼ëŠ”ê²ƒ\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  # num_layersë§Œí¼ ìŒ“ì•„ì˜¬ë¦° ì¸ì½”ë”ì˜ ì¸µ.\n",
    "  for i in range(num_layers):\n",
    "    outputs = encoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name=\"encoder_layer_{}\".format(i),\n",
    "    )([outputs, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, padding_mask], outputs=outputs, name=name)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc51a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë””ì½”ë” ì¸µ ë§Œë“¤ê¸°\n",
    "# ë””ì½”ë” í•˜ë‚˜ì˜ ë ˆì´ì–´ë¥¼ í•¨ìˆ˜ë¡œ êµ¬í˜„.\n",
    "# ì´ í•˜ë‚˜ì˜ ë ˆì´ì–´ ì•ˆì—ëŠ” ì„¸ ê°œì˜ ì„œë¸Œ ë ˆì´ì–´ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\n",
    "def decoder_layer(units, d_model, num_heads, dropout, name=\"decoder_layer\"):\n",
    "  inputs = tf.keras.Input(shape=(None, d_model), name=\"inputs\")\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name=\"encoder_outputs\")\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name=\"look_ahead_mask\")\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "\n",
    "  # ì²« ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì…€í”„ ì–´í…ì…˜)\n",
    "  attention1 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_1\")(inputs={\n",
    "          'query': inputs,\n",
    "          'key': inputs,\n",
    "          'value': inputs,\n",
    "          'mask': look_ahead_mask\n",
    "      })\n",
    "\n",
    "  # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ” LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "  attention1 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention1 + inputs)\n",
    "\n",
    "  # ë‘ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : ë§ˆìŠ¤í¬ë“œ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ ìˆ˜í–‰ (ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜)\n",
    "  attention2 = MultiHeadAttention(\n",
    "      d_model, num_heads, name=\"attention_2\")(inputs={\n",
    "          'query': attention1,\n",
    "          'key': enc_outputs,\n",
    "          'value': enc_outputs,\n",
    "          'mask': padding_mask\n",
    "      })\n",
    "\n",
    "  # ë§ˆìŠ¤í¬ë“œ ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì˜ ê²°ê³¼ëŠ”\n",
    "  # Dropoutê³¼ LayerNormalizationì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "  attention2 = tf.keras.layers.Dropout(rate=dropout)(attention2)\n",
    "  attention2 = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(attention2 + attention1)\n",
    "\n",
    "  # ì„¸ ë²ˆì§¸ ì„œë¸Œ ë ˆì´ì–´ : 2ê°œì˜ ì™„ì „ì—°ê²°ì¸µ\n",
    "  outputs = tf.keras.layers.Dense(units=units, activation='relu')(attention2)\n",
    "  outputs = tf.keras.layers.Dense(units=d_model)(outputs)\n",
    "\n",
    "  # ì™„ì „ì—°ê²°ì¸µì˜ ê²°ê³¼ëŠ” Dropoutê³¼ LayerNormalization ìˆ˜í–‰\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(outputs)\n",
    "  outputs = tf.keras.layers.LayerNormalization(\n",
    "      epsilon=1e-6)(outputs + attention2)\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f47fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ë””ì½”ë” ë§Œë“¤ê¸°\n",
    "def decoder(vocab_size,\n",
    "            num_layers,\n",
    "            units,\n",
    "            d_model,\n",
    "            num_heads,\n",
    "            dropout,\n",
    "            name='decoder'):\n",
    "  inputs = tf.keras.Input(shape=(None,), name='inputs')\n",
    "  enc_outputs = tf.keras.Input(shape=(None, d_model), name='encoder_outputs')\n",
    "  look_ahead_mask = tf.keras.Input(\n",
    "      shape=(1, None, None), name='look_ahead_mask')\n",
    "\n",
    "  # íŒ¨ë”© ë§ˆìŠ¤í¬\n",
    "  padding_mask = tf.keras.Input(shape=(1, 1, None), name='padding_mask')\n",
    "  \n",
    "  # ì„ë² ë”© ë ˆì´ì–´\n",
    "  embeddings = tf.keras.layers.Embedding(vocab_size, d_model)(inputs)\n",
    "  embeddings *= tf.math.sqrt(tf.cast(d_model, tf.float32))\n",
    "\n",
    "  # í¬ì§€ì…”ë„ ì¸ì½”ë”©\n",
    "  embeddings = PositionalEncoding(vocab_size, d_model)(embeddings)\n",
    "\n",
    "  # Dropoutì´ë¼ëŠ” í›ˆë ¨ì„ ë•ëŠ” í…Œí¬ë‹‰ì„ ìˆ˜í–‰\n",
    "  outputs = tf.keras.layers.Dropout(rate=dropout)(embeddings)\n",
    "\n",
    "  for i in range(num_layers):\n",
    "    outputs = decoder_layer(\n",
    "        units=units,\n",
    "        d_model=d_model,\n",
    "        num_heads=num_heads,\n",
    "        dropout=dropout,\n",
    "        name='decoder_layer_{}'.format(i),\n",
    "    )(inputs=[outputs, enc_outputs, look_ahead_mask, padding_mask])\n",
    "\n",
    "  return tf.keras.Model(\n",
    "      inputs=[inputs, enc_outputs, look_ahead_mask, padding_mask],\n",
    "      outputs=outputs,\n",
    "      name=name)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4545c771",
   "metadata": {},
   "source": [
    "í•„ìš”ì—†ì„ê±°ê°™ì€ ë¶€ë¶„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75435f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ìš©í•  ìƒ˜í”Œì˜ ìµœëŒ€ ê°œìˆ˜\n",
    "#MAX_SAMPLES = 50000\n",
    "#print(MAX_SAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d0c410",
   "metadata": {},
   "source": [
    "ì „ì²˜ë¦¬ í•¨ìˆ˜: ë°ì´í„°ì…‹ì— ë§ê²Œ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì¡°ì •í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "í•œê¸€ í…ìŠ¤íŠ¸ ì²˜ë¦¬ì— ì í•©í•˜ë„ë¡ ì •ê·œ í‘œí˜„ì‹ì„ ìˆ˜ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477f0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì§ˆë¬¸ê³¼ ë‹µë³€ì˜ ìŒì¸ ë°ì´í„°ì…‹ì„ êµ¬ì„±í•˜ê¸° ìœ„í•œ ë°ì´í„° ë¡œë“œ í•¨ìˆ˜\n",
    "def load_conversations():\n",
    "  id2line = {}\n",
    "  with open(path_to_movie_lines, errors='ignore') as file:\n",
    "    lines = file.readlines()\n",
    "  for line in lines:\n",
    "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "    id2line[parts[0]] = parts[4]\n",
    "\n",
    "  inputs, outputs = [], []\n",
    "  with open(path_to_movie_conversations, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "  for line in lines:\n",
    "    parts = line.replace('\\n', '').split(' +++$+++ ')\n",
    "    conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n",
    "\n",
    "    for i in range(len(conversation) - 1):\n",
    "      # ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ì§ˆë¬¸ì— í•´ë‹¹ë˜ëŠ” inputsì™€ ë‹µë³€ì— í•´ë‹¹ë˜ëŠ” outputsì— ì ìš©.\n",
    "      inputs.append(preprocess_sentence(id2line[conversation[i]]))\n",
    "      outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n",
    "\n",
    "      if len(inputs) >= MAX_SAMPLES:\n",
    "        return inputs, outputs\n",
    "  return inputs, outputs\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5b325d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conversations():\n",
    "    data_path = 'data/ChatbotData.csv'\n",
    "    data = pd.read_csv(data_path)\n",
    "    inputs, outputs = [], []\n",
    "    for _, row in data.iterrows():\n",
    "      inputs.append(preprocess_sentence(row['Q']))\n",
    "      outputs.append(preprocess_sentence(row['A']))\n",
    "    return inputs, outputs\n",
    "\n",
    "load_conversations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ee53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•˜ì—¬ ì§ˆë¬¸ì„ questions, ë‹µë³€ì„ answersì— ì €ì¥í•©ë‹ˆë‹¤.\n",
    "questions, answers = load_conversations()\n",
    "print('ì „ì²´ ìƒ˜í”Œ ìˆ˜ :', len(questions))\n",
    "print('ì „ì²´ ìƒ˜í”Œ ìˆ˜ :', len(answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32836c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ì „ì²˜ë¦¬ í›„ì˜ 22ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: {}'.format(questions[20]))\n",
    "print('ì „ì²˜ë¦¬ í›„ì˜ 22ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: {}'.format(answers[20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de522100",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2aac41",
   "metadata": {},
   "source": [
    "# Step 3. ëª¨ë¸ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë°ì´í„° ì¸ì½”ë”©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98ed013",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "print(\"ì‚´ì§ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆì–´ìš”. ìŠ¤íŠ¸ë ˆì¹­ í•œ ë²ˆ í•´ë³¼ê¹Œìš”? ğŸ‘\")\n",
    "\n",
    "# ì§ˆë¬¸ê³¼ ë‹µë³€ ë°ì´í„°ì…‹ì— ëŒ€í•´ì„œ Vocabulary ìƒì„±\n",
    "tokenizer = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(questions + answers, target_vocab_size=2**13)\n",
    "print(\"ìŠ=3 \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d701376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì— ê³ ìœ í•œ ì •ìˆ˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤.\n",
    "START_TOKEN, END_TOKEN = [tokenizer.vocab_size], [tokenizer.vocab_size + 1]\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e8c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('START_TOKENì˜ ë²ˆí˜¸ :' ,[tokenizer.vocab_size])\n",
    "print('END_TOKENì˜ ë²ˆí˜¸ :' ,[tokenizer.vocab_size + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8bf210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ê³ ë ¤í•˜ì—¬ +2ë¥¼ í•˜ì—¬ ë‹¨ì–´ì¥ì˜ í¬ê¸°ë¥¼ ì‚°ì •í•©ë‹ˆë‹¤.\n",
    "VOCAB_SIZE = tokenizer.vocab_size + 2\n",
    "print(VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbbc58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„ì˜ì˜ 22ë²ˆì§¸ ìƒ˜í”Œì— ëŒ€í•´ì„œ ì •ìˆ˜ ì¸ì½”ë”© ì‘ì—…ì„ ìˆ˜í–‰.\n",
    "# ê° í† í°ì„ ê³ ìœ í•œ ì •ìˆ˜ë¡œ ë³€í™˜\n",
    "print('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ì§ˆë¬¸ ìƒ˜í”Œ: {}'.format(tokenizer.encode(questions[21])))\n",
    "print('ì •ìˆ˜ ì¸ì½”ë”© í›„ì˜ 21ë²ˆì§¸ ë‹µë³€ ìƒ˜í”Œ: {}'.format(tokenizer.encode(answers[21])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0193c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒ˜í”Œì˜ ìµœëŒ€ í—ˆìš© ê¸¸ì´ ë˜ëŠ” íŒ¨ë”© í›„ì˜ ìµœì¢… ê¸¸ì´\n",
    "MAX_LENGTH = 40\n",
    "print(MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a4cbcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •ìˆ˜ ì¸ì½”ë”©, ìµœëŒ€ ê¸¸ì´ë¥¼ ì´ˆê³¼í•˜ëŠ” ìƒ˜í”Œ ì œê±°, íŒ¨ë”©\n",
    "def tokenize_and_filter(inputs, outputs):\n",
    "  tokenized_inputs, tokenized_outputs = [], []\n",
    "  \n",
    "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
    "    # ì •ìˆ˜ ì¸ì½”ë”© ê³¼ì •ì—ì„œ ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì¶”ê°€\n",
    "    sentence1 = START_TOKEN + tokenizer.encode(sentence1) + END_TOKEN\n",
    "    sentence2 = START_TOKEN + tokenizer.encode(sentence2) + END_TOKEN\n",
    "\n",
    "    # ìµœëŒ€ ê¸¸ì´ 40 ì´í•˜ì¸ ê²½ìš°ì—ë§Œ ë°ì´í„°ì…‹ìœ¼ë¡œ í—ˆìš©\n",
    "    if len(sentence1) <= MAX_LENGTH and len(sentence2) <= MAX_LENGTH:\n",
    "      tokenized_inputs.append(sentence1)\n",
    "      tokenized_outputs.append(sentence2)\n",
    "  \n",
    "  # ìµœëŒ€ ê¸¸ì´ 40ìœ¼ë¡œ ëª¨ë“  ë°ì´í„°ì…‹ì„ íŒ¨ë”©\n",
    "  tokenized_inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_inputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  tokenized_outputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "      tokenized_outputs, maxlen=MAX_LENGTH, padding='post')\n",
    "  \n",
    "  return tokenized_inputs, tokenized_outputs\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c81bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, answers = tokenize_and_filter(questions, answers)\n",
    "print('ë‹¨ì–´ì¥ì˜ í¬ê¸° :',(VOCAB_SIZE))\n",
    "print('í•„í„°ë§ í›„ì˜ ì§ˆë¬¸ ìƒ˜í”Œ ê°œìˆ˜: {}'.format(len(questions)))\n",
    "print('í•„í„°ë§ í›„ì˜ ë‹µë³€ ìƒ˜í”Œ ê°œìˆ˜: {}'.format(len(answers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb25639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# êµì‚¬ ê°•ìš”(Teacher Forcing) ì‚¬ìš©í•˜ê¸°\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "# ë””ì½”ë”ëŠ” ì´ì „ì˜ targetì„ ë‹¤ìŒì˜ inputìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "# ì´ì— ë”°ë¼ outputsì—ì„œëŠ” START_TOKENì„ ì œê±°í•˜ê² ìŠµë‹ˆë‹¤.\n",
    "dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        'inputs': questions,\n",
    "        'dec_inputs': answers[:, :-1]\n",
    "    },\n",
    "    {\n",
    "        'outputs': answers[:, 1:]\n",
    "    },\n",
    "))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a02865",
   "metadata": {},
   "source": [
    "Step 4. ëª¨ë¸ êµ¬ì„±í•˜ê¸°\n",
    "ìœ„ ì‹¤ìŠµ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03e8729",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(vocab_size,\n",
    "                num_layers,\n",
    "                units,\n",
    "                d_model,\n",
    "                num_heads,\n",
    "                dropout,\n",
    "                name=\"transformer\"):\n",
    "  inputs = tf.keras.Input(shape=(None,), name=\"inputs\")\n",
    "  dec_inputs = tf.keras.Input(shape=(None,), name=\"dec_inputs\")\n",
    "\n",
    "  # ì¸ì½”ë”ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬\n",
    "  enc_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='enc_padding_mask')(inputs)\n",
    "\n",
    "  # ë””ì½”ë”ì—ì„œ ë¯¸ë˜ì˜ í† í°ì„ ë§ˆìŠ¤í¬ í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "  # ë‚´ë¶€ì ìœ¼ë¡œ íŒ¨ë”© ë§ˆìŠ¤í¬ë„ í¬í•¨ë˜ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.\n",
    "  look_ahead_mask = tf.keras.layers.Lambda(\n",
    "      create_look_ahead_mask,\n",
    "      output_shape=(1, None, None),\n",
    "      name='look_ahead_mask')(dec_inputs)\n",
    "\n",
    "  # ë‘ ë²ˆì§¸ ì–´í…ì…˜ ë¸”ë¡ì—ì„œ ì¸ì½”ë”ì˜ ë²¡í„°ë“¤ì„ ë§ˆìŠ¤í‚¹\n",
    "  # ë””ì½”ë”ì—ì„œ íŒ¨ë”©ì„ ìœ„í•œ ë§ˆìŠ¤í¬\n",
    "  dec_padding_mask = tf.keras.layers.Lambda(\n",
    "      create_padding_mask, output_shape=(1, 1, None),\n",
    "      name='dec_padding_mask')(inputs)\n",
    "\n",
    "  # ì¸ì½”ë”\n",
    "  enc_outputs = encoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[inputs, enc_padding_mask])\n",
    "\n",
    "  # ë””ì½”ë”\n",
    "  dec_outputs = decoder(\n",
    "      vocab_size=vocab_size,\n",
    "      num_layers=num_layers,\n",
    "      units=units,\n",
    "      d_model=d_model,\n",
    "      num_heads=num_heads,\n",
    "      dropout=dropout,\n",
    "  )(inputs=[dec_inputs, enc_outputs, look_ahead_mask, dec_padding_mask])\n",
    "\n",
    "  # ì™„ì „ì—°ê²°ì¸µ\n",
    "  outputs = tf.keras.layers.Dense(units=vocab_size, name=\"outputs\")(dec_outputs)\n",
    "\n",
    "  return tf.keras.Model(inputs=[inputs, dec_inputs], outputs=outputs, name=name)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93afe8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ëª¨ë¸ìƒì„±\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "NUM_LAYERS = 2 # ì¸ì½”ë”ì™€ ë””ì½”ë”ì˜ ì¸µì˜ ê°œìˆ˜\n",
    "D_MODEL = 256 # ì¸ì½”ë”ì™€ ë””ì½”ë” ë‚´ë¶€ì˜ ì…, ì¶œë ¥ì˜ ê³ ì • ì°¨ì›\n",
    "NUM_HEADS = 8 # ë©€í‹° í—¤ë“œ ì–´í…ì…˜ì—ì„œì˜ í—¤ë“œ ìˆ˜ \n",
    "UNITS = 512 # í”¼ë“œ í¬ì›Œë“œ ì‹ ê²½ë§ì˜ ì€ë‹‰ì¸µì˜ í¬ê¸°\n",
    "DROPOUT = 0.1 # ë“œë¡­ì•„ì›ƒì˜ ë¹„ìœ¨\n",
    "\n",
    "model = transformer(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    units=UNITS,\n",
    "    d_model=D_MODEL,\n",
    "    num_heads=NUM_HEADS,\n",
    "    dropout=DROPOUT)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8b4af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì†ì‹¤í•¨ìˆ˜\n",
    "\n",
    "def loss_function(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  \n",
    "  loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "      from_logits=True, reduction='none')(y_true, y_pred)\n",
    "\n",
    "  mask = tf.cast(tf.not_equal(y_true, 0), tf.float32)\n",
    "  loss = tf.multiply(loss, mask)\n",
    "\n",
    "  return tf.reduce_mean(loss)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf8a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì»¤ìŠ¤í…€ ëœ í•™ìŠµë¥ \n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super(CustomSchedule, self).__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps**-1.5)\n",
    "\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c5a174",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_learning_rate = CustomSchedule(d_model=128)\n",
    "\n",
    "plt.plot(sample_learning_rate(tf.range(200000, dtype=tf.float32)))\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.xlabel(\"Train Step\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d729ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ëª¨ë¸ ì»´íŒŒì¼\n",
    "learning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "def accuracy(y_true, y_pred):\n",
    "  y_true = tf.reshape(y_true, shape=(-1, MAX_LENGTH - 1))\n",
    "  return tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss_function, metrics=[accuracy])\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f29d2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# í›ˆë ¨í•˜ê¸°\n",
    "\n",
    "EPOCHS = 50\n",
    "model.fit(dataset, epochs=EPOCHS, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921a05ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì±—ë´‡ í…ŒìŠ¤íŠ¸í•˜ê¸°\n",
    "\n",
    "def decoder_inference(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # ì…ë ¥ëœ ë¬¸ì¥ì„ ì •ìˆ˜ ì¸ì½”ë”© í›„, ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì•ë’¤ë¡œ ì¶”ê°€.\n",
    "  # ex) Where have you been? â†’ [[8331   86   30    5 1059    7 8332]]\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # ë””ì½”ë”ì˜ í˜„ì¬ê¹Œì§€ì˜ ì˜ˆì¸¡í•œ ì¶œë ¥ ì‹œí€€ìŠ¤ê°€ ì§€ì†ì ìœ¼ë¡œ ì €ì¥ë˜ëŠ” ë³€ìˆ˜.\n",
    "  # ì²˜ìŒì—ëŠ” ì˜ˆì¸¡í•œ ë‚´ìš©ì´ ì—†ìŒìœ¼ë¡œ ì‹œì‘ í† í°ë§Œ ë³„ë„ ì €ì¥. ex) 8331\n",
    "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # ë””ì½”ë”ì˜ ì¸í¼ëŸ°ìŠ¤ ë‹¨ê³„\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # ë””ì½”ë”ëŠ” ìµœëŒ€ MAX_LENGTHì˜ ê¸¸ì´ë§Œí¼ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
    "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ì˜ ì •ìˆ˜\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # ë§Œì•½ í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì´ë¼ë©´ forë¬¸ì„ ì¢…ë£Œ\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # ì˜ˆì¸¡í•œ ë‹¨ì–´ë“¤ì€ ì§€ì†ì ìœ¼ë¡œ output_sequenceì— ì¶”ê°€ë©ë‹ˆë‹¤.\n",
    "    # ì´ output_sequenceëŠ” ë‹¤ì‹œ ë””ì½”ë”ì˜ ì…ë ¥ì´ ë©ë‹ˆë‹¤.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b433f7b",
   "metadata": {},
   "source": [
    "# Step 5. ëª¨ë¸ í‰ê°€í•˜ê¸°\n",
    "Step 1ì—ì„œ ì„ íƒí•œ ì „ì²˜ë¦¬ ë°©ë²•ì„ ê³ ë ¤í•˜ì—¬ ì…ë ¥ëœ ë¬¸ì¥ì— ëŒ€í•´ì„œ ëŒ€ë‹µì„ ì–»ëŠ” ì˜ˆì¸¡ í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6edb5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ì˜ì˜ ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ì„œ decoder_inference() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì±—ë´‡ì˜ ëŒ€ë‹µì„ ì–»ëŠ” sentence_generation() í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n",
    "def sentence_generation(sentence):\n",
    "  # ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ì„œ ë””ì½”ë”ë¥¼ ë™ì‘ ì‹œì¼œ ì˜ˆì¸¡ëœ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë¦¬í„´ë°›ìŠµë‹ˆë‹¤.\n",
    "  prediction = decoder_inference(sentence)\n",
    "\n",
    "  # ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('ì…ë ¥ : {}'.format(sentence))\n",
    "  print('ì¶œë ¥ : {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91597d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_inference(sentence):\n",
    "  sentence = preprocess_sentence(sentence)\n",
    "\n",
    "  # ì…ë ¥ëœ ë¬¸ì¥ì„ ì •ìˆ˜ ì¸ì½”ë”© í›„, ì‹œì‘ í† í°ê³¼ ì¢…ë£Œ í† í°ì„ ì•ë’¤ë¡œ ì¶”ê°€.\n",
    "  # ex) Where have you been? â†’ [[8331   86   30    5 1059    7 8332]]\n",
    "  sentence = tf.expand_dims(\n",
    "      START_TOKEN + tokenizer.encode(sentence) + END_TOKEN, axis=0)\n",
    "\n",
    "  # ë””ì½”ë”ì˜ í˜„ì¬ê¹Œì§€ì˜ ì˜ˆì¸¡í•œ ì¶œë ¥ ì‹œí€€ìŠ¤ê°€ ì§€ì†ì ìœ¼ë¡œ ì €ì¥ë˜ëŠ” ë³€ìˆ˜.\n",
    "  # ì²˜ìŒì—ëŠ” ì˜ˆì¸¡í•œ ë‚´ìš©ì´ ì—†ìŒìœ¼ë¡œ ì‹œì‘ í† í°ë§Œ ë³„ë„ ì €ì¥. ex) 8331\n",
    "  output_sequence = tf.expand_dims(START_TOKEN, 0)\n",
    "\n",
    "  # ë””ì½”ë”ì˜ ì¸í¼ëŸ°ìŠ¤ ë‹¨ê³„\n",
    "  for i in range(MAX_LENGTH):\n",
    "    # ë””ì½”ë”ëŠ” ìµœëŒ€ MAX_LENGTHì˜ ê¸¸ì´ë§Œí¼ ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ì„ ë°˜ë³µí•©ë‹ˆë‹¤.\n",
    "    predictions = model(inputs=[sentence, output_sequence], training=False)\n",
    "    predictions = predictions[:, -1:, :]\n",
    "\n",
    "    # í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ì˜ ì •ìˆ˜\n",
    "    predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.int32)\n",
    "\n",
    "    # ë§Œì•½ í˜„ì¬ ì˜ˆì¸¡í•œ ë‹¨ì–´ê°€ ì¢…ë£Œ í† í°ì´ë¼ë©´ forë¬¸ì„ ì¢…ë£Œ\n",
    "    if tf.equal(predicted_id, END_TOKEN[0]):\n",
    "      break\n",
    "\n",
    "    # ì˜ˆì¸¡í•œ ë‹¨ì–´ë“¤ì€ ì§€ì†ì ìœ¼ë¡œ output_sequenceì— ì¶”ê°€ë©ë‹ˆë‹¤.\n",
    "    # ì´ output_sequenceëŠ” ë‹¤ì‹œ ë””ì½”ë”ì˜ ì…ë ¥ì´ ë©ë‹ˆë‹¤.\n",
    "    output_sequence = tf.concat([output_sequence, predicted_id], axis=-1)\n",
    "\n",
    "  return tf.squeeze(output_sequence, axis=0)\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4a8c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_generation(sentence):\n",
    "  # ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ì„œ ë””ì½”ë”ë¥¼ ë™ì‘ ì‹œì¼œ ì˜ˆì¸¡ëœ ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë¦¬í„´ë°›ìŠµë‹ˆë‹¤.\n",
    "  prediction = decoder_inference(sentence)\n",
    "\n",
    "  # ì •ìˆ˜ ì‹œí€€ìŠ¤ë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "  predicted_sentence = tokenizer.decode(\n",
    "      [i for i in prediction if i < tokenizer.vocab_size])\n",
    "\n",
    "  print('ì…ë ¥ : {}'.format(sentence))\n",
    "  print('ì¶œë ¥ : {}'.format(predicted_sentence))\n",
    "\n",
    "  return predicted_sentence\n",
    "print(\"ìŠ=3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d56a5d",
   "metadata": {},
   "source": [
    "í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4947c074",
   "metadata": {},
   "source": [
    "50í¬í¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "108e019e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : Where have you been?\n",
      "ì¶œë ¥ : ì‚¬ë‘ì€ ì•ˆ ë³€í•˜ê³  ì‚¬ëŒì´ ë³€í•´ìš” . \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ì‚¬ë‘ì€ ì•ˆ ë³€í•˜ê³  ì‚¬ëŒì´ ë³€í•´ìš” . '"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('Where have you been?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "7280f474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : ë„ˆë°”ë³´ë‹ˆ?\n",
      "ì¶œë ¥ : ë§ì´ ë§Œë‚  ìˆ˜ ìˆì„ ê±°ì˜ˆìš” . \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ë§ì´ ë§Œë‚  ìˆ˜ ìˆì„ ê±°ì˜ˆìš” . '"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('ë„ˆë°”ë³´ë‹ˆ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "d0a9bf43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : í•œê¸€ì€ ì–´ë ¤ì›Œ\n",
      "ì¶œë ¥ : ê°™ì´ ì—¬í–‰ì„ ë– ë‚˜ë³´ì„¸ìš” . \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ê°™ì´ ì—¬í–‰ì„ ë– ë‚˜ë³´ì„¸ìš” . '"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('í•œê¸€ì€ ì–´ë ¤ì›Œ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "5fd182c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : ë„ˆë°”ë³´ë‹ˆ?\n",
      "ì¶œë ¥ : ë§ì´ ë§Œë‚  ìˆ˜ ìˆì„ ê±°ì˜ˆìš” . \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ë§ì´ ë§Œë‚  ìˆ˜ ìˆì„ ê±°ì˜ˆìš” . '"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('ë„ˆë°”ë³´ë‹ˆ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "74435ff4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : ì €ëŠ” ìœ„ë¡œí•´ë“œë¦¬ëŠ” ë¡œë´‡ì´ì—ìš”\n",
      "ì¶œë ¥ : ë§ì´ ë‹¹í™©í–ˆê² ì–´ìš” . \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ë§ì´ ë‹¹í™©í–ˆê² ì–´ìš” . '"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('ì €ëŠ” ìœ„ë¡œí•´ë“œë¦¬ëŠ” ë¡œë´‡ì´ì—ìš”')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "238b67e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : 1\n",
      "ì¶œë ¥ : ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤ . \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤ . '"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "64266d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : 11\n",
      "ì¶œë ¥ : ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤ . \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ìœ„ë¡œí•´ ë“œë¦½ë‹ˆë‹¤ . '"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('11')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3c704bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : 2\n",
      "ì¶œë ¥ : í—¤ì–´ì§„ í•˜ë£¨ë¥¼ ì„¸ëŠ” ìì‹ ì„ ë” í˜ë“¤ê²Œ ë§Œë“¤ ë¿ì´ì—ìš” . \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'í—¤ì–´ì§„ í•˜ë£¨ë¥¼ ì„¸ëŠ” ìì‹ ì„ ë” í˜ë“¤ê²Œ ë§Œë“¤ ë¿ì´ì—ìš” . '"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d82f327b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : 22\n",
      "ì¶œë ¥ : í—¤ì–´ì§„ í•˜ë£¨ë¥¼ ì„¸ëŠ” ìì‹ ì„ ë” í˜ë“¤ê²Œ ë§Œë“¤ ë¿ì´ì—ìš” . \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'í—¤ì–´ì§„ í•˜ë£¨ë¥¼ ì„¸ëŠ” ìì‹ ì„ ë” í˜ë“¤ê²Œ ë§Œë“¤ ë¿ì´ì—ìš” . '"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('22')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "da177427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : 100\n",
      "ì¶œë ¥ : ìƒˆë¡œìš´ ë¬´ì–¸ê°€ë¥¼ í•´ë³´ëŠ”ê±´ ì–´ë– ì„¸ìš” . \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ìƒˆë¡œìš´ ë¬´ì–¸ê°€ë¥¼ í•´ë³´ëŠ”ê±´ ì–´ë– ì„¸ìš” . '"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c0de4c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : ì˜í™”\n",
      "ì¶œë ¥ : ì˜í™”ëŠ” í˜„ì‹¤ê³¼ ë‹¤ë¥´ì§€ë§Œ ê·¸ëŸ° ì‚¬ë‘ì„ í•˜ë„ë¡ ë…¸ë ¥í•´ë³´ë©´ ì˜í™” ê°™ì€ ì‚¬ë‘ í•  ìˆ˜ ìˆì„ê±°ì˜ˆìš” . \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ì˜í™”ëŠ” í˜„ì‹¤ê³¼ ë‹¤ë¥´ì§€ë§Œ ê·¸ëŸ° ì‚¬ë‘ì„ í•˜ë„ë¡ ë…¸ë ¥í•´ë³´ë©´ ì˜í™” ê°™ì€ ì‚¬ë‘ í•  ìˆ˜ ìˆì„ê±°ì˜ˆìš” . '"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('ì˜í™”')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "e1b2b983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : ìŒì‹ì \n",
      "ì¶œë ¥ : ì¸ìƒì€ ë˜ëŒì´í‘œì¸ê°€ë´ìš” . \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ì¸ìƒì€ ë˜ëŒì´í‘œì¸ê°€ë´ìš” . '"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('ìŒì‹ì ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "43576cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : ì§‘ì— ê°€ê³  ì‹¶ì–´\n",
      "ì¶œë ¥ : ì§‘ì´ ìµœê³ ì£  . \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ì§‘ì´ ìµœê³ ì£  . '"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('ì§‘ì— ê°€ê³  ì‹¶ì–´')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "a6e19fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì…ë ¥ : ë¶€ìê°€ ë˜ê³  ì‹¶ì–´\n",
      "ì¶œë ¥ : ê·¸ëŸ° ê±±ì •ì€ ì•ˆ í•˜ì…”ë„ ë˜ì–´ìš” . \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ê·¸ëŸ° ê±±ì •ì€ ì•ˆ í•˜ì…”ë„ ë˜ì–´ìš” . '"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_generation('ë¶€ìê°€ ë˜ê³  ì‹¶ì–´')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c571c70",
   "metadata": {},
   "source": [
    "### songys/Chatbot_data ì„ í™œìš©í•˜ì—¬ ì±—ë´‡ì„ ë§Œë“¤ì—ˆì–´\n",
    "ê²°ê³¼ë¥¼ ë³´ë‹ˆ ëŒ€í™”ê°€ ë™ë¬¸ì„œë‹µì¸ ëŠë‚Œì´ì•¼, ì´ë¥¼ ê°œì„ í•˜ê¸° ìœ„í•´ì„  ì–´ë– í•œ ë°©ë²•ë“¤ì´ ì¡´ì¬í• ê¹Œ?\n",
    "\n",
    "\n",
    "### í˜„ì¬ êµê³¼ì„œì²˜ëŸ¼ ë“¤ë¦¬ëŠ” ì±—ë´‡ì˜ ëŒ€í™” í’ˆì§ˆì„ í–¥ìƒí•˜ë ¤ë©´ ë‹¤ìŒ ì „ëµì„ ê³ ë ¤í•˜ì‹­ì‹œì˜¤.\n",
    "\n",
    "ë” ë§ì€ ëŒ€í™”í˜• ë°ì´í„° í†µí•©: ë”ìš± ë‹¤ì–‘í•˜ê³  ëŒ€í™”í˜•ì´ë©° ë¹„ê³µì‹ì ì¸ ë°ì´í„° ì†ŒìŠ¤ë¡œ êµìœ¡ ë°ì´í„° ì„¸íŠ¸ë¥¼ í™•ì¥í•˜ì—¬ ì±—ë´‡ì— ë³´ë‹¤ ìì—°ìŠ¤ëŸ¬ìš´ í†¤ì„ ì œê³µí•©ë‹ˆë‹¤. í˜„ì¬ ë°ì´í„° ì„¸íŠ¸ëŠ” ë„ˆë¬´ í˜•ì‹ì ì´ê±°ë‚˜ ë²”ìœ„ê°€ ì œí•œì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ìƒí™©ë³„ ì´í•´ ì‚¬ìš©: ë§¥ë½ì„ ì´í•´í•˜ê±°ë‚˜ ëŒ€í™” ìƒíƒœë¥¼ ìœ ì§€í•  ìˆ˜ ìˆëŠ” ì•Œê³ ë¦¬ì¦˜ì„ êµ¬í˜„í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì±—ë´‡ì€ ë³´ë‹¤ ê´€ë ¨ì„±ì´ ë†’ê³  ë§¤ë ¥ì ì¸ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "NLU(ìì—°ì–´ ì´í•´) í†µí•©: Google Dialogflow ë˜ëŠ” IBM Watsonê³¼ ê°™ì€ ë„êµ¬ëŠ” ì‚¬ìš©ì ì˜ë„ë¥¼ ë³´ë‹¤ íš¨ê³¼ì ìœ¼ë¡œ í•´ì„í•˜ëŠ” ì´í•´ ë° ì²˜ë¦¬ ê³„ì¸µì„ ì¶”ê°€í•˜ì—¬ ëŒ€í™”ê°€ ë³´ë‹¤ ìì—°ìŠ¤ëŸ½ê²Œ íë¥´ë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ê°ì • ë¶„ì„ ì‚¬ìš©: ê°ì •ì ì¸ ë‚´ìš©ì— ëŒ€í•œ ì‚¬ìš©ì ì…ë ¥ì„ ë¶„ì„í•˜ê³  ê³µê°ê³¼ ì ì ˆí•œ ê°ì •ì  ë°˜ì‘ì„ ë°˜ì˜í•˜ë„ë¡ ë°˜ì‘ì„ ë§ì¶¤í™”í•˜ì—¬ ìƒí˜¸ ì‘ìš©ì´ ë”ìš± ì¸ê°„ì ì¸ ëŠë‚Œì„ ê°–ë„ë¡ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "ë™ì  ì‘ë‹µ ìƒì„±: ë¯¸ë¦¬ ì •ì˜ëœ ì •ì ì¸ ì‘ë‹µ ëŒ€ì‹  ì‹ ê²½ë§ì´ë‚˜ ìƒì„± ëª¨ë¸ê³¼ ê°™ì€ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ ë™ì  ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤. GPT(Generative Pre-trained Transformer)ì™€ ê°™ì€ ëª¨ë¸ì€ ë”ìš± ë‹¤ì–‘í•˜ê³  ì¸ê°„ê³¼ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ì •ê¸° ì—…ë°ì´íŠ¸ ë° í”¼ë“œë°± ë£¨í”„: ìƒˆë¡œìš´ ë°ì´í„°ì™€ ì‚¬ìš©ì í”¼ë“œë°±ìœ¼ë¡œ ì±—ë´‡ì„ ì§€ì†ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•˜ì—¬ ì‘ë‹µì„ ê°œì„ í•˜ê³  ë³€í™”í•˜ëŠ” ì‚¬ìš©ì ê¸°ë³¸ ì„¤ì • ë° ì–¸ì–´ ì‚¬ìš©ì— ì ì‘í•©ë‹ˆë‹¤.\n",
    "\n",
    "ëŒ€í™” ì „ëµ ê°œì„ : ìœ ë¨¸ í¬í•¨, ë‹¤ì–‘í•œ ì‘ë‹µ ê¸¸ì´, ë³µì¡ì„±, ë°˜ë³µì ì¸ íŒ¨í„´ ë°©ì§€ ë“± í¥ë¯¸ë¡œìš´ ëŒ€í™”ë¥¼ ë§Œë“¤ê¸° ìœ„í•œ ì§€ì¹¨ì„ ì°¸ì¡°í•˜ì„¸ìš”[[4](https://www.phoneburner.com/ ë¸”ë¡œê·¸/ì¬ë¯¸ìˆëŠ” ëŒ€í™”ë¥¼ ë‚˜ëˆ„ëŠ” ë°©ë²•)].\n",
    "\n",
    "ì´ëŸ¬í•œ ë³€ê²½ ì‚¬í•­ì„ êµ¬í˜„í•¨ìœ¼ë¡œì¨ ì±—ë´‡ì€ ëŒ€í™”ì—ì„œ ë”ìš± ì ì‘ë ¥ì´ ë›°ì–´ë‚˜ê³  ë§¤ë ¥ì ì´ë©° êµê³¼ì„œì™€ ê°™ì§€ ì•Šê²Œ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2488d73",
   "metadata": {},
   "source": [
    "ë°°ìš´ ì  : ì±—ë´‡ì˜ ì›ë¦¬ì— ëŒ€í•´ì„œ ì•Œ ìˆ˜ ìˆì—ˆë‹¤.\n",
    "\n",
    "ì•„ì‰¬ìš´ ì  : ê²°ê³¼ë¬¼ì ìœ¼ë¡  ë””í…Œì¼í•œ ë‹µë³€ì´ ì•ˆë‚˜ì˜¤ëŠ”ê²Œ ì•„ì‰¬ì› ë‹¤\n",
    "\n",
    "ëŠë‚€ ì  : ì–´ë–»ê²Œí•˜ë©´ ë” ê¹Šì€ ëŒ€í™”ì™€ ì •í™•ë„ë¥¼ ì˜¬ë¦´ ìˆ˜ ìˆì„ì§€ ê³ ë¯¼ë˜ì—ˆë‹¤.\n",
    "\n",
    "ì–´ë ¤ì› ë˜ ì  : ë¶€ì¡±í•œ ì½”ë”© ì‹¤ë ¥ì— csví˜¸ì¶œí•˜ê³  ì—°ê²°í•˜ëŠ” ê³¼ì •ì´ ì–´ë ¤ì› ë‹¤\n",
    "í˜¼ìì„œ ì´ ë¬¸ì œë§Œ 2-3ì‹œê°„ ë„˜ê²Œ í•´ê²°í•´ë³´ì•˜ëŠ”ë° íŒ€ì›ì˜ ë„ì›€ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ í•´ê²°ë˜ì—ˆë‹¤\n",
    "ê°„ë‹¨í•œ ê²ƒì—ì„œ ë¶€ë”ªì¹˜ì§€ ì•Šë„ë¡ ì—¬ëŸ¬ ìƒí™©ì„ ê²ªì–´ë´ì•¼ê² ë‹¤"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
